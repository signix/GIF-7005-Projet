{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a5f25db",
   "metadata": {},
   "source": [
    "# Prédiction de l’achalandage des stations Bixi\n",
    "\n",
    "Ce projet vise à prédire le taux d’entrées et de sorties par tranche horaire dans les stations Bixi à l’aide de plusieurs modèles :\n",
    "\n",
    "- **STGNN (Spatio-Temporal Graph Neural Network)** : réseau neuronal à graphes spatio-temporels, stations comme nœuds, arêtes basées sur la distance ou le temps de déplacement.\n",
    "- **LSTM (Long Short-Term Memory)** : réseau neuronal récurrent adapté aux séries temporelles.\n",
    "- **Modèle statistique** : régression ou analyse classique pour servir de baseline.\n",
    "\n",
    "Nous comparerons la performance de ces modèles sur les données d’achalandage.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Importation des librairies et des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4cf9073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# Pour apprentissage\n",
    "#import torch\n",
    "#import torch.nn as nn\n",
    "#from torch.utils.data import DataLoader, Dataset\n",
    "# Pour LSTM\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import LSTM, Dense\n",
    "# Pour le modèle statistique\n",
    "import statsmodels.api as sm\n",
    "# Pour le graphe\n",
    "# import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953819d6",
   "metadata": {},
   "source": [
    "## 2. Exploration et préparation des données\n",
    "\n",
    "- Nettoyage et visualisation\n",
    "\n",
    "- Construction du graphe des stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5e37f89-8863-4fa5-83d9-f7a10d77bcb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        end_date  end_station_code  \\\n",
      "start_date          start_station_code                               \n",
      "2018-08-23 21:00:00 6184                      14                14   \n",
      "2018-09-04 17:00:00 6184                      12                12   \n",
      "2018-08-03 11:00:00 6131                      12                12   \n",
      "2018-07-19 23:15:00 6155                      12                12   \n",
      "2018-06-27 17:30:00 6184                      11                11   \n",
      "...                                          ...               ...   \n",
      "2018-11-15 23:45:00 6155                       1                 1   \n",
      "                    6158                       1                 1   \n",
      "                    6164                       1                 1   \n",
      "                    6169                       1                 1   \n",
      "2018-04-10 12:30:00 6237                       1                 1   \n",
      "\n",
      "                                        duration_sec  \n",
      "start_date          start_station_code                \n",
      "2018-08-23 21:00:00 6184                          14  \n",
      "2018-09-04 17:00:00 6184                          12  \n",
      "2018-08-03 11:00:00 6131                          12  \n",
      "2018-07-19 23:15:00 6155                          12  \n",
      "2018-06-27 17:30:00 6184                          11  \n",
      "...                                              ...  \n",
      "2018-11-15 23:45:00 6155                           1  \n",
      "                    6158                           1  \n",
      "                    6164                           1  \n",
      "                    6169                           1  \n",
      "2018-04-10 12:30:00 6237                           1  \n",
      "\n",
      "[303263 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/merged2018_reduced_dataset.csv\", parse_dates=['start_date', 'end_date'])\n",
    "df['start_date'] = pd.to_datetime(df['start_date'])\n",
    "# print(df.get(0))\n",
    "df_grouped = df.groupby([ pd.Grouper(key=\"start_date\", freq=\"15min\"),\n",
    "                 pd.Grouper('start_station_code')\n",
    "                 ]).count()\n",
    "\n",
    "print(df_grouped.sort_values(by=['end_date'], ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4544560c",
   "metadata": {},
   "source": [
    "## 3. Modélisation\n",
    "\n",
    "### 3.1 Modèle statistique (baseline) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14e14061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.4473305597783349\n",
      "RMSE: 0.6870355561229784\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/merged2018_reduced_dataset.csv\") \n",
    "df['start_date'] = pd.to_datetime(df['start_date'])\n",
    "df['end_date'] = pd.to_datetime(df['end_date'])\n",
    "df['slot_15'] = df['end_date'].dt.floor('15T') # Tronquer à l'intervalle de 15 minutes le plus proche\n",
    "\n",
    "# Calcul des arrivées par station et par intervalle de 15 minutes\n",
    "arrivals = (\n",
    "    df\n",
    "    .groupby(['end_station_code', 'slot_15'])\n",
    "    .size()\n",
    "    .reset_index(name='arrivals')\n",
    ")\n",
    "\n",
    "# Calcul de la moyenne historique des arrivées par station, jour de la semaine, heure et minute\n",
    "arrivals['dow'] = arrivals['slot_15'].dt.dayofweek   # 0=lundi\n",
    "arrivals['hour'] = arrivals['slot_15'].dt.hour     # 0-23\n",
    "arrivals['minute'] = arrivals['slot_15'].dt.minute   # 0,15,30,45\n",
    "\n",
    "# Calcul de la moyenne historique des arrivées\n",
    "historical_mean = (\n",
    "    arrivals\n",
    "    .groupby(['end_station_code', 'dow', 'hour', 'minute'])['arrivals']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={'arrivals': 'mean_arrivals'})\n",
    ")\n",
    "# Fusion des données réelles avec les moyennes historiques\n",
    "pred = arrivals.merge(\n",
    "    historical_mean,\n",
    "    on=['end_station_code', 'dow', 'hour', 'minute'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "# Calcul des métriques MAE et RMSE\n",
    "mae = mean_absolute_error(pred['arrivals'], pred['mean_arrivals'])\n",
    "rmse = np.sqrt(mean_squared_error(pred['arrivals'], pred['mean_arrivals']))\n",
    "print(\"MAE:\", mae)\n",
    "print(\"RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce1decbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.4814\n",
      "RMSE: 0.6990\n",
      "Nombre de prédictions: 65662\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "# Chargement et préparation\n",
    "df = pd.read_csv(\"data/merged2018_reduced_dataset.csv\") \n",
    "df['start_date'] = pd.to_datetime(df['start_date'])\n",
    "df['end_date'] = pd.to_datetime(df['end_date'])\n",
    "df['slot_15'] = df['end_date'].dt.floor('15T')\n",
    "\n",
    "# SPLIT TEMPOREL SANS LEAKAGE\n",
    "# Trier par date pour un split temporel propre\n",
    "df = df.sort_values('end_date')\n",
    "\n",
    "# Split temporel (80% train, 20% test)\n",
    "split_date = df['end_date'].quantile(0.8)\n",
    "train_df = df[df['end_date'] < split_date]\n",
    "test_df = df[df['end_date'] >= split_date]\n",
    "\n",
    "# Calcul des moyennes historiques SUR LE TRAIN SEULEMENT\n",
    "train_arrivals = (\n",
    "    train_df\n",
    "    .groupby(['end_station_code', 'slot_15'])\n",
    "    .size()\n",
    "    .reset_index(name='arrivals')\n",
    ")\n",
    "\n",
    "train_arrivals['dow'] = train_arrivals['slot_15'].dt.dayofweek\n",
    "train_arrivals['hour'] = train_arrivals['slot_15'].dt.hour\n",
    "train_arrivals['minute'] = train_arrivals['slot_15'].dt.minute\n",
    "\n",
    "historical_mean = (\n",
    "    train_arrivals\n",
    "    .groupby(['end_station_code', 'dow', 'hour', 'minute'])['arrivals']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={'arrivals': 'mean_arrivals'})\n",
    ")\n",
    "\n",
    "# Prédictions sur le TEST\n",
    "test_arrivals = (\n",
    "    test_df\n",
    "    .groupby(['end_station_code', 'slot_15'])\n",
    "    .size()\n",
    "    .reset_index(name='true_arrivals')\n",
    ")\n",
    "\n",
    "test_arrivals['dow'] = test_arrivals['slot_15'].dt.dayofweek\n",
    "test_arrivals['hour'] = test_arrivals['slot_15'].dt.hour\n",
    "test_arrivals['minute'] = test_arrivals['slot_15'].dt.minute\n",
    "\n",
    "# Fusion avec moyennes historiques\n",
    "pred = test_arrivals.merge(\n",
    "    historical_mean,\n",
    "    on=['end_station_code', 'dow', 'hour', 'minute'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Gestion des valeurs manquantes (pas dans le train)\n",
    "# Moyenne globale de la station\n",
    "station_global_mean = train_arrivals.groupby('end_station_code')['arrivals'].mean()\n",
    "\n",
    "def fill_missing(row):\n",
    "    if pd.isna(row['mean_arrivals']):\n",
    "        return station_global_mean.get(row['end_station_code'], 0)\n",
    "    return row['mean_arrivals']\n",
    "\n",
    "pred['predicted_arrivals'] = pred.apply(fill_missing, axis=1)\n",
    "\n",
    "# Évaluation\n",
    "mae = mean_absolute_error(pred['true_arrivals'], pred['predicted_arrivals'])\n",
    "rmse = np.sqrt(mean_squared_error(pred['true_arrivals'], pred['predicted_arrivals']))\n",
    "\n",
    "print(f\"MAE:  {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"Nombre de prédictions: {len(pred)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd44a703",
   "metadata": {},
   "source": [
    "### 3.2 Modèle statistique SARIMA et ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b3c7cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Station 6017 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bineta\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:581: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  warnings.warn('A date index has been provided, but it has no'\n",
      "c:\\Users\\Bineta\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:581: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  warnings.warn('A date index has been provided, but it has no'\n",
      "c:\\Users\\Bineta\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:581: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  warnings.warn('A date index has been provided, but it has no'\n",
      "c:\\Users\\Bineta\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:376: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  warnings.warn('No supported index is available.'\n",
      "c:\\Users\\Bineta\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:581: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  warnings.warn('A date index has been provided, but it has no'\n",
      "c:\\Users\\Bineta\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:581: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  warnings.warn('A date index has been provided, but it has no'\n",
      "c:\\Users\\Bineta\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:376: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  warnings.warn('No supported index is available.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARIMA  -> MAE=0.283, RMSE=0.424\n",
      "SARIMA -> MAE=0.327, RMSE=0.452\n",
      "\n",
      " Résultats\n",
      "   station  ARIMA_MAE  ARIMA_RMSE  SARIMA_MAE  SARIMA_RMSE\n",
      "0     6017   0.283268    0.423796    0.326751     0.451709\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "df = pd.read_csv(\"data/merged2018_reduced_dataset.csv\")\n",
    "\n",
    "df['end_date'] = pd.to_datetime(df['end_date'])\n",
    "df['slot_15'] = df['end_date'].dt.floor('15T')\n",
    "\n",
    "arrivals = (\n",
    "    df.groupby(['end_station_code', 'slot_15'])\n",
    "      .size()\n",
    "      .reset_index(name='arrivals')\n",
    ")\n",
    "\n",
    "\n",
    "stations_to_test = [6017]     \n",
    "\n",
    "\n",
    "def train_arima(ts):\n",
    "    train_size = int(len(ts) * 0.8)\n",
    "    train, test = ts[:train_size], ts[train_size:]\n",
    "\n",
    "    model = ARIMA(train, order=(1,0,1)).fit()\n",
    "    preds = model.forecast(len(test))\n",
    "\n",
    "    mae = mean_absolute_error(test, preds)\n",
    "    rmse = np.sqrt(mean_squared_error(test, preds))\n",
    "    return mae, rmse\n",
    "\n",
    "\n",
    "def train_sarima(ts, season=96):\n",
    "    \"\"\"\n",
    "    96 = 24h * (4 slots per hour)\n",
    "    \"\"\"\n",
    "    train_size = int(len(ts) * 0.8)\n",
    "    train, test = ts[:train_size], ts[train_size:]\n",
    "\n",
    "    model = SARIMAX(\n",
    "        train,\n",
    "        order=(1,0,1),\n",
    "        seasonal_order=(1,0,1,season),\n",
    "        enforce_stationarity=False,\n",
    "        enforce_invertibility=False\n",
    "    ).fit(disp=False)\n",
    "\n",
    "    preds = model.forecast(len(test))\n",
    "\n",
    "    mae = mean_absolute_error(test, preds)\n",
    "    rmse = np.sqrt(mean_squared_error(test, preds))\n",
    "    return mae, rmse\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for st in stations_to_test:\n",
    "    print(f\"\\n Station {st} \")\n",
    "\n",
    "    station_df = arrivals[arrivals['end_station_code'] == st].copy()\n",
    "    station_df = station_df.set_index(\"slot_15\").sort_index()\n",
    "\n",
    "    ts = station_df['arrivals']\n",
    "\n",
    "    if len(ts) < 200:\n",
    "        print(\"Pas assez de données → skip\")\n",
    "        continue\n",
    "\n",
    "    mae_a, rmse_a = train_arima(ts)\n",
    "    mae_s, rmse_s = train_sarima(ts)\n",
    "\n",
    "    print(f\"ARIMA  -> MAE={mae_a:.3f}, RMSE={rmse_a:.3f}\")\n",
    "    print(f\"SARIMA -> MAE={mae_s:.3f}, RMSE={rmse_s:.3f}\")\n",
    "\n",
    "    results.append({\n",
    "        \"station\": st,\n",
    "        \"ARIMA_MAE\": mae_a,\n",
    "        \"ARIMA_RMSE\": rmse_a,\n",
    "        \"SARIMA_MAE\": mae_s,\n",
    "        \"SARIMA_RMSE\": rmse_s,\n",
    "    })\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n Résultats\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79206a3f",
   "metadata": {},
   "source": [
    "### 3.2 Modèle LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b6aa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 0\n",
      "TensorFlow version: 2.20.0\n",
      "Num GPUs Available: 0\n",
      "cpu\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\olidr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 0.0096 - val_loss: 0.0053\n",
      "Epoch 2/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 0.0096 - val_loss: 0.0053\n",
      "Epoch 2/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0093 - val_loss: 0.0052\n",
      "Epoch 3/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0093 - val_loss: 0.0052\n",
      "Epoch 3/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0093 - val_loss: 0.0052\n",
      "Epoch 4/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0093 - val_loss: 0.0052\n",
      "Epoch 4/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0093 - val_loss: 0.0053\n",
      "Epoch 5/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0093 - val_loss: 0.0053\n",
      "Epoch 5/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0093 - val_loss: 0.0053\n",
      "Epoch 6/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0093 - val_loss: 0.0053\n",
      "Epoch 6/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0093 - val_loss: 0.0056\n",
      "Epoch 7/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0093 - val_loss: 0.0056\n",
      "Epoch 7/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0093 - val_loss: 0.0054\n",
      "Epoch 8/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0093 - val_loss: 0.0054\n",
      "Epoch 8/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0093 - val_loss: 0.0052\n",
      "Epoch 9/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0093 - val_loss: 0.0052\n",
      "Epoch 9/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0093 - val_loss: 0.0054\n",
      "Epoch 10/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0093 - val_loss: 0.0054\n",
      "Epoch 10/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0093 - val_loss: 0.0053\n",
      "Epoch 11/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0093 - val_loss: 0.0053\n",
      "Epoch 11/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0093 - val_loss: 0.0056\n",
      "Epoch 12/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0093 - val_loss: 0.0056\n",
      "Epoch 12/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0092 - val_loss: 0.0053\n",
      "Epoch 13/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0092 - val_loss: 0.0053\n",
      "Epoch 13/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0093 - val_loss: 0.0053\n",
      "Epoch 14/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0093 - val_loss: 0.0053\n",
      "Epoch 14/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0093 - val_loss: 0.0054\n",
      "Epoch 15/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0093 - val_loss: 0.0054\n",
      "Epoch 15/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0093 - val_loss: 0.0053\n",
      "Epoch 16/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0093 - val_loss: 0.0053\n",
      "Epoch 16/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0092 - val_loss: 0.0054\n",
      "Epoch 17/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0092 - val_loss: 0.0054\n",
      "Epoch 17/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0092 - val_loss: 0.0054\n",
      "Epoch 18/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0092 - val_loss: 0.0054\n",
      "Epoch 18/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0092 - val_loss: 0.0053\n",
      "Epoch 19/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0092 - val_loss: 0.0053\n",
      "Epoch 19/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0092 - val_loss: 0.0054\n",
      "Epoch 20/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0092 - val_loss: 0.0054\n",
      "Epoch 20/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0092 - val_loss: 0.0052\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0092 - val_loss: 0.0052\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "MAE LSTM: 0.599482371103625\n",
      "RMSE LSTM: 0.7229003745895571\n",
      "MAE LSTM: 0.599482371103625\n",
      "RMSE LSTM: 0.7229003745895571\n"
     ]
    }
   ],
   "source": [
    "# Modèle LSTM pour la prédiction des arrivées par station\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "# Exemple : prédire pour une station donnée (ex : 6169)\n",
    "station_id = 6169\n",
    "df_station = arrivals[arrivals['end_station_code'] == station_id].sort_values('slot_15')\n",
    "\n",
    "# Série temporelle des arrivées\n",
    "series = df_station['arrivals'].values.reshape(-1, 1)\n",
    "\n",
    "# Normalisation\n",
    "scaler = MinMaxScaler()\n",
    "series_scaled = scaler.fit_transform(series)\n",
    "\n",
    "# Création des séquences pour LSTM\n",
    "def create_sequences(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i+seq_length])\n",
    "        y.append(data[i+seq_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "seq_length = 96  # 24h si 15min pas step\n",
    "X, y = create_sequences(series_scaled, seq_length)\n",
    "\n",
    "# Split train/test\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "# Modèle LSTM\n",
    "model = Sequential([\n",
    "    LSTM(32, input_shape=(seq_length, 1)),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Entraînement\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "# Prédiction et inverse transform\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_inv = scaler.inverse_transform(y_pred)\n",
    "y_test_inv = scaler.inverse_transform(y_test)\n",
    "\n",
    "# Évaluation\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_inv, y_pred_inv))\n",
    "print('MAE LSTM:', mae)\n",
    "print('RMSE LSTM:', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d52f3ec",
   "metadata": {},
   "source": [
    "### 3.3 Modèle STGNN (Spatio-Temporal Graph Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafc87d9",
   "metadata": {},
   "source": [
    "## 4. Comparaison des performances\n",
    "\n",
    "- Métriques : RMSE, MAE, etc.\n",
    "- Visualisation des résultats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
